import pandas as pd
import numpy as np
import sklearn
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb


Train_Dataset = pd.read_csv("application_train.csv")
Test_Dataset = pd.read_csv("application_test.csv")
Bureau = pd.read_csv("bureau.csv")
#POS_CASH = pd.read_csv("POS_CASH_balance.csv")
Prev_App = pd.read_csv("previous_application.csv")


# Dimensions of all datasets
print("The shape of TRAIN dataset is:", Train_Dataset.shape)
print("The shape of TEST dataset is:", Test_Dataset.shape)
print("The shape of BUREAU dataset is:", Bureau.shape)
#print("The shape of POS_CASH dataset is:", POS_CASH.shape)
print("The shape of PREV APPLICATION dataset is:", Prev_App.shape)


## Joining Datasets
# Joining numerical variables to Train
cols = Bureau.drop(['SK_ID_BUREAU'], axis = 1).groupby(by=['SK_ID_CURR']).mean().reset_index()
cols.columns = ['Bureau_'+column if column !='SK_ID_CURR' else column for column in cols.columns]
App_Train_Buea = Train_Dataset.merge(cols, on='SK_ID_CURR', how='left')

# Joining numerical variables to Test
cols = Bureau.drop(['SK_ID_BUREAU'], axis = 1).groupby(by=['SK_ID_CURR']).mean().reset_index()
cols.columns = ['Bureau_'+column if column !='SK_ID_CURR' else column for column in cols.columns]
App_Test_Buea = Test_Dataset.merge(cols, on='SK_ID_CURR', how='left')

# Joining categorical variables to Train
Bureau_Cat= pd.get_dummies(Bureau.select_dtypes('object'))
Bureau_Cat['SK_ID_CURR'] = Bureau['SK_ID_CURR']
cols = Bureau_Cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()
cols.columns = ['Bureau_'+column if column !='SK_ID_CURR' else column for column in cols.columns]
App_Train_Buea = App_Train_Buea.merge(cols, on='SK_ID_CURR', how='left')

# Joining categorical variables to Test
Bureau_Cat= pd.get_dummies(Bureau.select_dtypes('object'))
Bureau_Cat['SK_ID_CURR'] = Bureau['SK_ID_CURR']
cols = Bureau_Cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()
cols.columns = ['Bureau_'+column if column !='SK_ID_CURR' else column for column in cols.columns]
App_Test_Buea = App_Test_Buea.merge(cols, on='SK_ID_CURR', how='left')

# Dimensions of Train and Test after joining
print('The shape App_Train_Buea data combined:',App_Train_Buea.shape)
print('The shape App_Test_Buea data combined:',App_Test_Buea.shape)

# Calculating Count of previous applications per customer
cols = Prev_App[['SK_ID_CURR','SK_ID_PREV']].groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].count().reset_index().rename(columns={'SK_ID_PREV':'Prev_App_Count'})
App_Train_Buea_Prev = App_Train_Buea.merge(cols, on =['SK_ID_CURR'], how = 'left')

cols = Prev_App[['SK_ID_CURR','SK_ID_PREV']].groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].count().reset_index().rename(columns={'SK_ID_PREV':'Prev_App_Count'})
App_Test_Buea_Prev = App_Test_Buea.merge(cols, on =['SK_ID_CURR'], how = 'left')

# Joining numerical variables to Train
cols = Prev_App.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()
prev_columns = ['Prev_App'+column if column != 'SK_ID_CURR' else column for column in cols.columns ]
cols.columns = prev_columns
App_Train_Buea_Prev = App_Train_Buea_Prev.merge(cols, on =['SK_ID_CURR'], how = 'left')


# Joining numerical variables to Test
cols = Prev_App.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()
prev_columns = ['Prev_App'+column if column != 'SK_ID_CURR' else column for column in cols.columns ]
cols.columns = prev_columns
App_Test_Buea_Prev = App_Test_Buea_Prev.merge(cols, on =['SK_ID_CURR'], how = 'left')

# Joining numerical variables to Train
prev_categorical = pd.get_dummies(Prev_App.select_dtypes('object'))
prev_categorical['SK_ID_CURR'] = Prev_App['SK_ID_CURR']
prev_categorical.head()
cols = prev_categorical.groupby('SK_ID_CURR').mean().reset_index()
cols.columns = ['Prev_App'+column if column != 'SK_ID_CURR' else column for column in cols.columns]
App_Train_Buea_Prev = App_Train_Buea_Prev.merge(cols, on=['SK_ID_CURR'], how='left')


# Joining categorical variables to Train
prev_categorical = pd.get_dummies(Prev_App.select_dtypes('object'))
prev_categorical['SK_ID_CURR'] = Prev_App['SK_ID_CURR']
prev_categorical.head()
cols = prev_categorical.groupby('SK_ID_CURR').mean().reset_index()
cols.columns = ['Prev_App'+column if column != 'SK_ID_CURR' else column for column in cols.columns]
App_Train_Buea_Prev = App_Train_Buea_Prev.merge(cols, on=['SK_ID_CURR'], how='left')


# Joining categorical variables to Test
prev_categorical = pd.get_dummies(Prev_App.select_dtypes('object'))
prev_categorical['SK_ID_CURR'] = Prev_App['SK_ID_CURR']
prev_categorical.head()
cols = prev_categorical.groupby('SK_ID_CURR').mean().reset_index()
cols.columns = ['Prev_App'+column if column != 'SK_ID_CURR' else column for column in cols.columns]
App_Test_Buea_Prev = App_Test_Buea_Prev.merge(cols, on=['SK_ID_CURR'], how='left')


# Dimensions of App_Train_Buea_Prev & App_Test_Buea_Prev
print('The shape App_Train_Buea_Prev data combined:',App_Train_Buea_Prev.shape)
print('The shape App_Test_Buea_Prev data combined:',App_Test_Buea_Prev.shape)

Train_Dataset = App_Train_Buea_Prev
Test_Dataset = App_Test_Buea_Prev

# **Exploratory Data Analysis**

After getting basic understanding of the Train and Test datasets. Now, let's do some EDA to understand the variables. Both univariate and multivariate analysis is done to understand the variables and their relationship with the target variables

print("The shape of TRAIN dataset is:", Train_Dataset.shape)
print("The shape of TEST dataset is:", Test_Dataset.shape)

Train_Dataset.info(max_cols = 500)

print("The TARGET value percentage distribution is: \n",round((Train_Dataset.TARGET.value_counts()/Train_Dataset.shape[0])*100,2))
plt.hist(Train_Dataset.TARGET)
plt.show()

plt.figure(figsize=(15,7))
plt.title("Histogram of Credit Amount");plt.xlabel("Credit Amount");
ax = sns.distplot(Train_Dataset["AMT_CREDIT"])

g = sns.FacetGrid(Train_Dataset, col="TARGET")
g.map(plt.hist, "AMT_CREDIT");

plt.figure(figsize=(15,7))
plt.title("Histogram of Amount of Goods Price")
plt.xlabel("Goods Price")
ax = sns.distplot(Train_Dataset["AMT_GOODS_PRICE"])

g = sns.FacetGrid(Train_Dataset, col="TARGET")
g.map(plt.hist, "AMT_GOODS_PRICE");

plt.figure(figsize = (10, 12))

# iterate through the sources
for i, ext_source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):
    
    # create a new subplot for each source
    plt.subplot(3, 1, i + 1)
    sns.distplot(Train_Dataset.loc[Train_Dataset['TARGET'] == 0, ext_source], label = 'TARGET = 0')
    sns.distplot(Train_Dataset.loc[Train_Dataset['TARGET'] == 1, ext_source], label = 'TARGET = 1')
    
    plt.title('Distribution of %s by Target Value' % ext_source)
    plt.xlabel('%s' % ext_source); plt.ylabel('Density');
    
plt.tight_layout(h_pad = 2.5)

sns.pairplot(Train_Dataset, vars=["AMT_INCOME_TOTAL","AMT_CREDIT", "AMT_ANNUITY"], hue="TARGET", height=2.5);

# Find the extent of missing values in variables

In order to model TARGET we need to understand the extent of missing values in predictors. We will try to use an appropriate technique for imputing missing value in these predictors based on the percentage share of missing values in each predictor column

fig = plt.figure(figsize=(18,6))
miss_train = pd.DataFrame((Train_Dataset.isnull().sum())*100/Train_Dataset.shape[0]).reset_index()
miss_test = pd.DataFrame((Test_Dataset.isnull().sum())*100/Test_Dataset.shape[0]).reset_index()
miss_train["type"] = "Training Data"
miss_test["type"]  =  "Test Data"
missing = pd.concat([miss_train,miss_test],axis=0)
ax = sns.pointplot("index",0,data=missing,hue="type")
plt.xticks(rotation =90,fontsize =7)
plt.title("Share of Missing Values")
plt.ylabel("Share in %")
plt.xlabel("Column Names")

# **Missing Value treatment**

Drop the columns with missing values greater than 50%, using these columns can be misleading as more than 50% of the values would be imputed, if chosen to impute the missing values. Hence, dropping such columns is a better option. Also, this is done separately in both train and test dataset and then common columns are selected for further analysis.

Train_Dataset = Train_Dataset.dropna(thresh = Train_Dataset.shape[0] * 0.5, how = 'all', axis = 1)
Test_Dataset = Test_Dataset.dropna(thresh = Test_Dataset.shape[0] * 0.5, how = 'all', axis = 1)
print("Train Shape" ,Train_Dataset.shape)
print("Test Shape" ,Test_Dataset.shape)

# Keep the intersection of columns
train_labels = Train_Dataset['TARGET']
Test_Dataset, Train_Dataset = Test_Dataset.align(Train_Dataset, join = 'inner', axis = 1)
print("After Aligning -> Train Shape" ,Train_Dataset.shape)
print("After Aligning -> Test Shape" ,Test_Dataset.shape)
# Add target back in to the data
Train_Dataset['TARGET'] = train_labels

print("Train Shape" ,Train_Dataset.shape)
print("Test Shape" ,Test_Dataset.shape)

fig = plt.figure(figsize=(18,6))
miss_train = pd.DataFrame((Train_Dataset.isnull().sum())*100/Train_Dataset.shape[0]).reset_index()
miss_test = pd.DataFrame((Test_Dataset.isnull().sum())*100/Test_Dataset.shape[0]).reset_index()
miss_train["type"] = "Training Data"
miss_test["type"]  =  "Test Data"
missing = pd.concat([miss_train,miss_test],axis=0)
ax = sns.pointplot("index",0,data=missing,hue="type")
plt.xticks(rotation =90,fontsize =7)
plt.title("Share of Missing Values")
plt.ylabel("Share in %")
plt.xlabel("Column Names")

# **Outliers and Anomalies Treatment**

Next step is outlier treatment and correcting the anomalies. These can be detected by looking at summary statistics of each column. We notice Days_birth are negative (as they are recorded relative to the current loan application) and need to be divided by -365 to get correct age. Also, the Days_Employed has outliers (max value is approx. 1000 years) which needs to treated by imputing these with median value of the column as the variable is right skewed and imputing with mean would be misleading

Train_Dataset.describe()

Train_Dataset['DAYS_BIRTH'] = (Train_Dataset['DAYS_BIRTH'] / -365)
Train_Dataset['DAYS_BIRTH'].describe()
Test_Dataset['DAYS_BIRTH'] = (Test_Dataset['DAYS_BIRTH'] / -365)
Test_Dataset['DAYS_BIRTH'].describe()

# Create an anomalous flag column
Train_Dataset['DAYS_EMPLOYED_ANOM'] = Train_Dataset["DAYS_EMPLOYED"] == 365243
Test_Dataset['DAYS_EMPLOYED_ANOM'] = Test_Dataset["DAYS_EMPLOYED"] == 365243

# Replace the anomalous values with nan
Train_Dataset['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)
Test_Dataset["DAYS_EMPLOYED"].replace({365243: np.nan}, inplace = True)

Train_Dataset['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');
plt.xlabel('Days Employment');

Test_Dataset['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');
plt.xlabel('Days Employment');


print('There are %d anomalies in the test data out of %d entries' % (Train_Dataset["DAYS_EMPLOYED_ANOM"].sum(), len(Train_Dataset)))
print('There are %d anomalies in the test data out of %d entries' % (Test_Dataset["DAYS_EMPLOYED_ANOM"].sum(), len(Test_Dataset)))

# **Feature Engineering**

There are features that can be engineered to improve the prediction. These features can be mathematical or based on domain knowledge. These can be introduced to see if they have good correaltion and causation with the target variable. These features/predictors can help immensely in improving the prediction

## FEATURES FOR TEST DATASET
# Domain based variables in test dataset
Test_Dataset['CREDIT_INCOME_PERCENT'] = Test_Dataset['AMT_CREDIT'] / Test_Dataset['AMT_INCOME_TOTAL']
Test_Dataset['ANNUITY_INCOME_PERCENT'] = Test_Dataset['AMT_ANNUITY'] / Test_Dataset['AMT_INCOME_TOTAL']
Test_Dataset['CREDIT_TERM'] = Test_Dataset['AMT_ANNUITY'] / Test_Dataset['AMT_CREDIT']
Test_Dataset['DAYS_EMPLOYED_PERCENT'] = Test_Dataset['DAYS_EMPLOYED'] / Test_Dataset['DAYS_BIRTH']
Test_Dataset['PAYMENT_RATE'] = Test_Dataset['AMT_ANNUITY'] / Test_Dataset['AMT_CREDIT']

# Featured variables from  Bureau Dataset
# Per customer the count of prev loans
cols = Bureau.groupby(by = ['SK_ID_CURR'])['SK_ID_BUREAU'].count().reset_index().rename(columns = {'SK_ID_BUREAU': 'BUREAU_LOAN_COUNT'})
Test_Dataset = Test_Dataset.merge(cols, on='SK_ID_CURR', how='left')

# Per customer count of types of loans
cols = Bureau[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by = ['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(columns={'CREDIT_TYPE': 'BUREAU_LOAN_TYPES'})
Test_Dataset = Test_Dataset.merge(cols, on='SK_ID_CURR', how='left')
Test_Dataset['BUREAU_LOAN_TYPES'] = Test_Dataset['BUREAU_LOAN_TYPES'].fillna(0)

# Ratio of Debt to Credit
Bureau['AMT_CREDIT_SUM'] = Bureau['AMT_CREDIT_SUM'].fillna(0)
Bureau['AMT_CREDIT_SUM_DEBT'] = Bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)
cols1 = Bureau[['SK_ID_CURR','AMT_CREDIT_SUM']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM': 'TOTAL_CREDIT_SUM'})
cols2 = Bureau[['SK_ID_CURR','AMT_CREDIT_SUM_DEBT']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_DEBT':'TOTAL_CREDIT_SUM_DEBT'})
cols1['DEBT_CREDIT_RATIO'] = cols2['TOTAL_CREDIT_SUM_DEBT']/cols1['TOTAL_CREDIT_SUM']

Test_Dataset = Test_Dataset.merge(cols1, on='SK_ID_CURR', how='left')
Test_Dataset['DEBT_CREDIT_RATIO'] = Test_Dataset['DEBT_CREDIT_RATIO'].fillna(0)
Test_Dataset['DEBT_CREDIT_RATIO'] = Test_Dataset.replace([np.inf, -np.inf], 0)
Test_Dataset['DEBT_CREDIT_RATIO'] = pd.to_numeric(Test_Dataset['DEBT_CREDIT_RATIO'], downcast='float')


## FEATURES FOR TRAIN DATASET
# Domain based variables in train dataset
Train_Dataset['CREDIT_INCOME_PERCENT'] = Train_Dataset['AMT_CREDIT'] / Train_Dataset['AMT_INCOME_TOTAL']
Train_Dataset['ANNUITY_INCOME_PERCENT'] = Train_Dataset['AMT_ANNUITY'] / Train_Dataset['AMT_INCOME_TOTAL']
Train_Dataset['CREDIT_TERM'] = Train_Dataset['AMT_ANNUITY'] / Train_Dataset['AMT_CREDIT']
Train_Dataset['DAYS_EMPLOYED_PERCENT'] = Train_Dataset['DAYS_EMPLOYED'] / Train_Dataset['DAYS_BIRTH']
Train_Dataset['PAYMENT_RATE'] = Train_Dataset['AMT_ANNUITY'] / Train_Dataset['AMT_CREDIT']

# Featured variables from  Bureau Dataset
# Per customer the count of prev loans
cols = Bureau.groupby(by = ['SK_ID_CURR'])['SK_ID_BUREAU'].count().reset_index().rename(columns = {'SK_ID_BUREAU': 'BUREAU_LOAN_COUNT'})
Train_Dataset = Train_Dataset.merge(cols, on='SK_ID_CURR', how='left')

# Per customer count of types of loans
cols = Bureau[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by = ['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(columns={'CREDIT_TYPE': 'BUREAU_LOAN_TYPES'})
Train_Dataset = Train_Dataset.merge(cols, on='SK_ID_CURR', how='left')
Train_Dataset['BUREAU_LOAN_TYPES'] = Train_Dataset['BUREAU_LOAN_TYPES'].fillna(0)

# Ratio of Debt to Credit
Bureau['AMT_CREDIT_SUM'] = Bureau['AMT_CREDIT_SUM'].fillna(0)
Bureau['AMT_CREDIT_SUM_DEBT'] = Bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)
cols1 = Bureau[['SK_ID_CURR','AMT_CREDIT_SUM']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM': 'TOTAL_CREDIT_SUM'})
cols2 = Bureau[['SK_ID_CURR','AMT_CREDIT_SUM_DEBT']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_DEBT':'TOTAL_CREDIT_SUM_DEBT'})
cols1['DEBT_CREDIT_RATIO'] = cols2['TOTAL_CREDIT_SUM_DEBT']/cols1['TOTAL_CREDIT_SUM']

Train_Dataset = Train_Dataset.merge(cols1, on='SK_ID_CURR', how='left')
Train_Dataset['DEBT_CREDIT_RATIO'] = Train_Dataset['DEBT_CREDIT_RATIO'].fillna(0)
Train_Dataset['DEBT_CREDIT_RATIO'] = Train_Dataset.replace([np.inf, -np.inf], 0)
Train_Dataset['DEBT_CREDIT_RATIO'] = pd.to_numeric(Train_Dataset['DEBT_CREDIT_RATIO'], downcast='float')





# **Dealing with Numerical and Categorical Variables**

The type of predictor variables will also determine how will they be treated. The categorical variables need to be one-hot encoded to be used as predictors in most of the ML algorithms. However, some of the algorithms can handle categorical variables as it is (without one-hot encoded) but we will encode them to make it constant

Train_Dataset.dtypes.value_counts()
Train_Dataset.select_dtypes(include=[object]).apply(pd.Series.nunique, axis = 0)

# one-hot encoding of categorical variables
Train_Dataset = pd.get_dummies(Train_Dataset)
Test_Dataset = pd.get_dummies(Test_Dataset)

print('Train Shape: ', Train_Dataset.shape)
print('Test Shape: ', Test_Dataset.shape)

train_labels = Train_Dataset['TARGET']
Test_Dataset, Train_Dataset = Test_Dataset.align(Train_Dataset, join = 'inner', axis = 1)
print("After Aligning -> Train Shape" ,Train_Dataset.shape)
print("After Aligning -> Test Shape" ,Test_Dataset.shape)
# Add target back in to the data
Train_Dataset['TARGET'] = train_labels

print("Train Shape" ,Train_Dataset.shape)
print("Test Shape" ,Test_Dataset.shape)

# **Missing Value Imputation and Variable Scaling**

Before implementing a model the missing values have to be imputed and then as a good practice the variables need to be scaled as well for the model to be general and give equal weightage to all variables based of thier scaled values irrespective of thier actual values.

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler

# Drop the target from the training data
if 'TARGET' in Train_Dataset:
    train = Train_Dataset.drop(columns = ['TARGET'])
else:
    train = Train_Dataset.copy()
    
# Feature names
features = list(train.columns)

# Copy of the testing data
#test = Test_Dataset.copy()

# Median imputation of missing values
imputer = SimpleImputer(strategy = 'median')

# Scale each feature to 0-1
scaler = MinMaxScaler(feature_range = (0, 1))

# Fit on the training data
imputer.fit(train)

# Transform both training and testing data
train = imputer.transform(train)
test = imputer.transform(Test_Dataset)

# Repeat with the scaler
scaler.fit(train)
train = scaler.transform(train)
test = scaler.transform(test)

print('Training data shape: ', train.shape)
print('Testing data shape: ', test.shape)


# **ML Model for predicting default probability**

Now we train the model on the training dataset and build a classifier to classify with probabilities whether a loan should be given to an applicant or no

# Modelling with Light GBM
from lightgbm import LGBMClassifier
clf = LGBMClassifier(
            n_estimators=4000,
            learning_rate=0.03,
            num_leaves=30,
            colsample_bytree=.8,
            subsample=.9,
            max_depth=7,
            reg_alpha=.1,
            reg_lambda=.1,
            min_split_gain=.01,
            min_child_weight=2,
            silent=-1,
            verbose=-1,
        )
        
clf.fit(train, train_labels,eval_metric='auc', verbose=100)


## First Tried with RF but commenting it out as LightGBM performs better
#from sklearn.ensemble import RandomForestClassifier

# Make the random forest classifier
#random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)

# Train on the training data
#random_forest.fit(train, train_labels)

# Make predictions on the test data
#predictions = random_forest.predict_proba(test)[:, 1]

